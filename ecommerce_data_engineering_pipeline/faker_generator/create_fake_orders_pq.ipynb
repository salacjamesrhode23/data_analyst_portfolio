{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ca3c41-70c7-4341-a874-d41c22b21bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb6ba54-7fcc-4309-8080-dfc071929013",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_location = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0035a3b-a472-44d4-99ad-96679e99fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster(\"spark://vm-instance-nyc-taxi.asia-southeast1-a.c.de-project-nyc-taxi.internal:7077\") \\\n",
    "    .setAppName(\"data_generator\") \\\n",
    "    .set(\"spark.jars\",\"/home/salacjamesrhode23/connectors/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"google.cloud.auth.service.account.json.keyfile\", creds_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19259582-313d-42b7-8fcd-3788ca8a14a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: google.cloud.auth.service.account.enable\n",
      "Warning: Ignoring non-Spark config property: google.cloud.auth.service.account.json.keyfile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/13 10:10:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", creds_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce88c49-3e40-4187-a746-85f97387f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e71b7bf-a354-440f-a7d2-01d85b4b4f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://vm-instance-nyc-taxi.asia-southeast1-a.c.de-project-nyc-taxi.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://vm-instance-nyc-taxi.asia-southeast1-a.c.de-project-nyc-taxi.internal:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data_generator</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x796b4deab7d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abfdd56-6d09-41ca-a838-9cf68d028407",
   "metadata": {},
   "source": [
    "### Generating Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a475e13-d5fa-458c-b9d0-0f9f12975b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4683e77b-a67e-4c24-bbb3-864264d4c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv files to dataframes\n",
    "project_path = os.getenv(\"ecomm\")\n",
    "\n",
    "customer_csv_path = os.path.join(project_path, \"faker_dataset\", \"faker_csv\", \"fake_customers.csv\")\n",
    "product_csv_path = os.path.join(project_path, \"faker_dataset\", \"faker_csv\", \"fake_products.csv\")\n",
    "output_path = \"gs://ecomm_bucket001/output_files/from_faker\"\n",
    "\n",
    "df_customers = pd.read_csv(customer_csv_path) \n",
    "df_products = pd.read_csv(product_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48f30c53-78a1-46f9-87ff-621f6a3e276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of customers and products\n",
    "customers_list = df_customers['Full Name'].tolist()\n",
    "products_list = df_products['Title'].tolist()\n",
    "\n",
    "# Define payment methods\n",
    "payment_methods = [\"PayPal\",\"Digital Wallet\",\"Cash on Delivery\",\"Bank Transfer\"]\n",
    "\n",
    "# Define inclusive dates for the fake data\n",
    "start = datetime(2010, 1, 1, 0, 0, 0, 0)\n",
    "end = datetime(2014, 12, 31, 0, 0, 0, 0)\n",
    "\n",
    "delta = end - start\n",
    "number_of_days = delta.days\n",
    "\n",
    "# Calculate the number of rows\n",
    "average_daily_transaction = 800\n",
    "number_of_rows = number_of_days*average_daily_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e193183-5d12-451e-99bf-55a28179aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460000\n",
      "1825\n"
     ]
    }
   ],
   "source": [
    "print(number_of_rows)\n",
    "print(number_of_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f37c50-c558-4cb2-8168-5e1334354e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "fake = Faker('en_PH')\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aa55878-fd82-408d-969b-f0528e913bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/11/13 10:32:02 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/11/13 10:32:02 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:218)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:923)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "25/11/13 10:32:02 ERROR TaskSchedulerImpl: Lost executor 0 on 10.148.0.2: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "Elapsed time: 5512.09 seconds\n",
      "Estimated total data size: 4.0817 GB\n",
      "Free storage before: 4.3779 GB\n",
      "Free storage after: 4.1581 GB\n"
     ]
    }
   ],
   "source": [
    "# Start timer and check free storage in GB\n",
    "free_gb_start = shutil.disk_usage(\"/\").free / (1024 ** 3)\n",
    "start_time = time.time()\n",
    "\n",
    "# Define a function to generate random reference/order numbers\n",
    "def random_number():\n",
    "    return f\"#{random.randint(100000000000, 999999999999)}\"\n",
    "\n",
    "# --- Generate Orders ---\n",
    "for _ in range(number_of_rows):\n",
    "    order_number = random_number()\n",
    "    order_date = fake.date_time_between_dates(datetime_start=start, datetime_end=end)\n",
    "    billing_name = random.choice(customers_list)\n",
    "    payment_method = random.choice(payment_methods)\n",
    "    payment_reference = random_number()\n",
    "\n",
    "    # Each order has 1â€“3 products (line items)\n",
    "    for _ in range(random.randint(1, 3)):\n",
    "        lineitem_name = random.choice(products_list)\n",
    "        lineitem_qty = random.randint(1, 3)\n",
    "\n",
    "        # Merge customer info\n",
    "        customer_info = df_customers.loc[df_customers['Full Name'] == billing_name].to_dict('records')[0]\n",
    "        # Merge product info\n",
    "        product_info = df_products.loc[df_products['Title'] == lineitem_name].to_dict('records')[0]\n",
    "\n",
    "        order_dict = {\n",
    "            'order_number': order_number,\n",
    "            'order_date': order_date,\n",
    "            'year': order_date.year,\n",
    "            'billing_name': billing_name,\n",
    "            'lineitem_name': lineitem_name,\n",
    "            'lineitem_qty': lineitem_qty,\n",
    "            'payment_method': payment_method,\n",
    "            'payment_reference': payment_reference,\n",
    "            'payment_date': order_date + timedelta(days=random.uniform(0, 1)),\n",
    "            'fulfillment_date': order_date + timedelta(days=random.uniform(1, 2)),\n",
    "        }\n",
    "\n",
    "        # Merge additional customer & product fields\n",
    "        order_dict.update({k.lower().replace(' ','_'): v for k, v in customer_info.items() if k != 'Full Name'})\n",
    "        order_dict.update({k.lower().replace(' ','_'): v for k, v in product_info.items() if k != 'Title'})\n",
    "\n",
    "        data.append(order_dict)\n",
    "\n",
    "# --- Compute Metrics ---\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Estimate total data size in GB using pandas for better accuracy\n",
    "df_estimate = pd.DataFrame(data)\n",
    "total_bytes = df_estimate.memory_usage(deep=True).sum()\n",
    "total_gb = total_bytes / (1024 ** 3)\n",
    "\n",
    "# Check free storage in GB\n",
    "free_gb_end = shutil.disk_usage(\"/\").free / (1024 ** 3)\n",
    "\n",
    "# --- Print Summary ---\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Estimated total data size: {total_gb:.4f} GB\")\n",
    "print(f\"Free storage before: {free_gb_start:.4f} GB\")\n",
    "print(f\"Free storage after: {free_gb_end:.4f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f615472-7897-4553-a91d-0c0029b4c71d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mbreak\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f3475-fac9-4c4d-b10d-ff40988cd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframe to spark dataframe\n",
    "df_orders = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf41566-5c9b-4816-9cf1-2dee0d5c6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a994ef-a280-4101-b8b7-5d2059537337",
   "metadata": {},
   "source": [
    "# WORKAROUND - DELETE WHEN IN PRODUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40d08533-7245-416e-a5d8-41708538104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03800f20-b251-4905-ae99-de97707a69fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime columns to microsecond precision before saving\n",
    "df_orders['order_date'] = df_orders['order_date'].astype('datetime64[us]')\n",
    "df_orders['payment_date'] = df_orders['payment_date'].astype('datetime64[us]')\n",
    "df_orders['fulfillment_date'] = df_orders['fulfillment_date'].astype('datetime64[us]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aff6485c-1d57-4702-93c9-e3ce8a15c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.to_parquet(\"pq/part-00003-8b63187c-abf5-40d4-afe6-e5f6c289e8fe-c000.snappy.parquet\", engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ba274-7b95-4733-a0e6-b051ae9e9023",
   "metadata": {},
   "source": [
    "### Parquet file names:\n",
    "part-00000-8b63187c-abf5-40d4-afe6-e5f6c289e8fe-c000.snappy.parquet <br>\n",
    "part-00001-8b63187c-abf5-40d4-afe6-e5f6c289e8fe-c000.snappy.parquet <br>\n",
    "part-00002-8b63187c-abf5-40d4-afe6-e5f6c289e8fe-c000.snappy.parquet <br>\n",
    "part-00003-8b63187c-abf5-40d4-afe6-e5f6c289e8fe-c000.snappy.parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03fb38db-f5ce-45b5-b142-634f52f77da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "## Now we can read the parquet file and load it to GCS\n",
    "df_orders = spark.read.parquet(\"pq/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e521c8c9-0626-4562-925b-4a9b34ce6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"gs://ecomm_bucket001/output_files/from_faker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54452525-45d0-4cf6-adac-03cf8a615184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_orders.repartition(15).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709305c0-7700-4048-873d-7b70dccb08ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
